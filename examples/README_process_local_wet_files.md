# Processing Local WET.gz Files

This document provides instructions on how to use the `process_local_wet_files.py` script to process Common Crawl WET files locally using a `datatrove` pipeline with Ray.

## Dependencies

### Main Dependencies
The script relies on `datatrove` and `ray`.

*   **Ray**: If you haven't installed `ray` with the necessary extras, you can install it via pip. A common set of extras is:
    ```bash
    pip install "ray[data,train,tune]"
    ```
    A more minimal installation like `pip install ray` might be sufficient, but `ray[data]` is often recommended for data processing workloads. Please refer to the official Ray documentation for the most suitable installation for your setup.

*   **Datatrove**: Ensure `datatrove` is installed. If you installed it from source or via pip, it should handle its core dependencies.

### WarcReader Dependencies
The `WarcReader` used in the script has specific dependencies:
*   `warcio`: For reading WARC/WET files.
*   `cchardet`: For fast character encoding detection.
*   `python-magic`: For identifying file types (used by some underlying components).

These are typically installed as dependencies of `datatrove`. However, it's good to be aware of them, especially if you encounter any issues related to file reading or encoding.

## Running the Script

You can run the script from the root of the `datatrove` repository.

### Example Command

```bash
python examples/process_local_wet_files.py <your_wet_files_directory> <your_output_directory> en --ray_tasks 4 --logging_dir ./my_pipeline_logs
```

### Argument Explanations

*   **Positional Arguments**:
    *   `<your_wet_files_directory>`: (Required) Replace this with the path to the directory where your `.wet.gz` (or other specified pattern) files are stored.
    *   `<your_output_directory>`: (Required) Replace this with the path to the directory where the processed JSONL files will be saved.
    *   `language`: (Required) One or more language codes (e.g., `en`, or `en es fr`) to filter the documents. The script will process documents matching any of these languages. Defaults to `en` if not specified, but it's a required argument in the command structure.

*   **Optional Arguments**:
    *   `--glob_pattern`: (Optional) The glob pattern to find WET files within your input directory.
        *   Default: `*.wet.gz`
        *   Example: `*.wet` for uncompressed WET files, or `data/**/*.wet.gz` for recursive search.
    *   `--ray_tasks`: (Optional) The number of parallel Ray tasks to use for processing.
        *   Default: `1`
        *   Adjust this based on your machine's capacity (e.g., number of CPU cores).
    *   `--logging_dir`: (Optional) The directory where pipeline execution logs (specifically Ray logs) will be stored.
        *   Default: `./pipeline_logs`

## Output Structure

The script will generate output in the specified directories:

*   **Filtered Output**:
    *   Location: `<your_output_directory>/filtered_<lang1>_<lang2>.../`
    *   Content: JSONL files containing documents that matched the specified language(s). For example, if you run with `en es`, the folder will be `filtered_en_es`.

*   **Excluded Output (from Language Filter)**:
    *   Location: `<your_output_directory>/non_<lang1>_<lang2>.../`
    *   Content: JSONL files containing documents that did *not* match the specified language(s) and were thus excluded by the `LanguageFilter`.

*   **Logs**:
    *   Location: `<logging_dir>/ray_logs/`
    *   Content: Log files generated by the Ray executor during the pipeline run. These are useful for debugging and monitoring progress.

Make sure the output directories and logging directory are writable. If they don't exist, the script (or Ray) will attempt to create them.
